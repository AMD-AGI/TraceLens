{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "795106a4",
   "metadata": {},
   "source": [
    "<!--\n",
    "Copyright (c) 2024 - 2025 Advanced Micro Devices, Inc. All rights reserved.\n",
    "\n",
    "See LICENSE for license information.\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6694ba",
   "metadata": {},
   "source": [
    "# JAX NCCL Analyser Example\n",
    "\n",
    "This notebook demonstrates how to use the `JaxNcclAnalyser` class to analyze bandwidth performance of collective communication operations in JAX traces.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The `JaxNcclAnalyser` provides functionality to:\n",
    "- Load and parse JAX trace files (protobuf format)\n",
    "- Extract collective communication events (all-reduce, all-gather, etc.)\n",
    "- Calculate bandwidth metrics for each collective operation\n",
    "- Analyze performance across different collective types\n",
    "- Generate summary statistics \n",
    "\n",
    "## Tracelens installation\n",
    "\n",
    "For detailed installation instructions, dependencies, and setup requirements, please refer to the [TraceLens README.md](https://github.com/ROCm/TraceLens/blob/main/README.md) in the main repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b7f449",
   "metadata": {},
   "source": [
    "## Step 0: Import Required Libraries\n",
    "\n",
    "Import the necessary modules for JAX NCCL analysis and data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b815a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TraceLens import JaxNcclAnalyser\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bafd77c",
   "metadata": {},
   "source": [
    "## Step 1: Configure Input Parameters\n",
    "\n",
    "## Trace Analysis Configuration\n",
    "\n",
    "This section configures the parameters for analyzing distributed training traces from multiple nodes.\n",
    "\n",
    "### Key Configuration Steps:\n",
    "\n",
    "1. **Set trace directory**: Point to the root directory containing all trace files\n",
    "2. **Configure world size**: Number of processes participating in distributed training setup (typically one process per GPU) \n",
    "3. **Create node-to-protobuf mapping**: Maps each node rank to its `.xplane.pb` trace file path\n",
    "\n",
    "### Expected Directory Structure\n",
    "\n",
    "The `traces_dir` should contain trace files organized by nodes, typically with this structure:\n",
    "\n",
    "```\n",
    "traces_dir/\n",
    "├── node_0/\n",
    "│   ├── plugins/\n",
    "│   │   └── profile/\n",
    "│   │       └── xyz/\n",
    "│   │           └── xyz.xplane.pb\n",
    "│   └── xla_dumps/\n",
    "│       └── module_xyz.jit_train_step.xxxxx_gpu_after_optimizations.txt\n",
    "├── node_1/\n",
    "│   ├── plugins/\n",
    "│   │   └── profile/\n",
    "│   │       └── xyz/\n",
    "│   │           └── xyz.xplane.pb\n",
    "│   └── xla_dumps/\n",
    "│       └── module_xyz.jit_train_step.xxxxx_gpu_after_optimizations.txt\n",
    "└── ...\n",
    "```\n",
    "\n",
    "**Note**: The exact paths may vary depending on how you collected the traces. \n",
    "\n",
    "### Node Rank Mapping\n",
    "\n",
    "The node rank determines how GPU ranks are calculated using the formula: \n",
    "`global_gpu_rank = node_rank * gpus_per_node + local_gpu_id`\n",
    "\n",
    "### Mapping Options:\n",
    "\n",
    "**Option 1: Auto-generation** (Use with caution)\n",
    "- Utility script available but may be incorrect for your specific setup\n",
    "- Always verify the generated mapping matches your actual configuration\n",
    "\n",
    "**Option 2: Manual mapping** (Recommended)\n",
    "- Ensures accuracy by explicitly defining each node's trace file path\n",
    "- Guarantees correct node rank assignment\n",
    "\n",
    "### Common Configurations:\n",
    "- **32 GPUs**: 4 nodes × 8 GPUs/node (MI300X nodes)\n",
    "- **64 GPUs**: 8 nodes × 8 GPUs/node  (MI300X nodes)\n",
    "- **128 GPUs**: 16 nodes × 8 GPUs/node (MI300X nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf4011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure trace analysis parameters\n",
    "traces_dir = \"/path/to/your/trace/files/directory\"\n",
    "# Modify world_size as per your case\n",
    "world_size = 32  #Number of processes participating in distributed training setup (typically one process per GPU)\n",
    "\n",
    "# Option 1: Auto-generate mapping (verify output)\n",
    "from TraceLens.NcclAnalyser.util import node_rank_to_protobuf_file_mapping\n",
    "# node_to_pb_file_mapping, total_nodes = node_rank_to_protobuf_file_mapping.get_node_rank_protobuf_mapping(traces_dir)\n",
    "\n",
    "# Option 2: Manual mapping (recommended)\n",
    "node_to_pb_file_mapping = {\n",
    "    # 0: \"/path/to/traces_dir/node_0/plugins/profile/xyz/xyz.xplane.pb\",\n",
    "    # 1: \"/path/to/traces_dir/node_1/plugins/profile/xyz/xyz.xplane.pb\",\n",
    "    # 2: \"/path/to/traces_dir/node_2/plugins/profile/xyz/xyz.xplane.pb\",\n",
    "    # ... continue for all nodes\n",
    "}\n",
    "\n",
    "# Validation\n",
    "gpus_per_node = 8\n",
    "expected_nodes = world_size // gpus_per_node\n",
    "\n",
    "print(f\"Traces directory: {traces_dir}\")\n",
    "print(f\"World size: {world_size} GPUs\")\n",
    "print(f\"Expected nodes: {expected_nodes} (assuming {gpus_per_node} GPUs per node)\")\n",
    "print(f\"Configured trace files: {len(node_to_pb_file_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b17c8",
   "metadata": {},
   "source": [
    "## Step 2: Initialize the Analyser\n",
    "\n",
    "Create an instance of the `JaxNcclAnalyser` class. This will automatically load and parse the trace files.\n",
    "\n",
    "### What Happens During Initialization\n",
    "\n",
    "The `JaxNcclAnalyser` constructor automatically performs several key initialization steps:\n",
    "\n",
    "1. **Load and parse trace data** from protobuf files using `JaxTraceToTree`\n",
    "2. **Extract NCCL/RCCL communication events** from the trace data\n",
    "3. **Build collective information datframe** by parsing XLA dump files\n",
    "\n",
    "### XLA dumps parsing\n",
    "\n",
    "The `JaxNcclAnalyser` uses an integrated XLA parser to extract collective operation metadata from XLA dump files, providing crucial information about replica groups, data sizes, and operation types for accurate bandwidth calculations.\n",
    "\n",
    "#### Expected Directory Structure\n",
    "\n",
    "The analyser expects XLA dump files in this structure:\n",
    "```\n",
    "traces_dir/\n",
    "├── node_0/\n",
    "│   ├── plugins/profile/xyz/xyz.xplane.pb\n",
    "│   └── xla_dumps/\n",
    "│       └── *jit_train_step.xxxxx_gpu_after_optimizations.txt\n",
    "├── node_1/\n",
    "│   ├── plugins/profile/xyz/xyz.xplane.pb  \n",
    "│   └── xla_dumps/\n",
    "│       └── *jit_train_step.xxxxx_gpu_after_optimizations.txt\n",
    "└── ...\n",
    "```\n",
    "\n",
    "#### Automatic XLA parsing\n",
    "\n",
    "**Important**: The analyser automatically builds XLA file mapping by:\n",
    "- Scanning `traces_dir` for XLA dump files matching `*jit_train_step.gfx942_gpu_after_optimizations.txt`\n",
    "- Creating node-to-XLA-file mapping based on directory structure similarity\n",
    "- No manual `node_to_xla_file_map` parameter required\n",
    "\n",
    "**Manual XLA File Mapping (Optional)**: If the automatic detection doesn't work for your directory structure, you can manually specify the `node_to_xla_file_map` parameter when creating the `JaxNcclAnalyser` instance:\n",
    "```python\n",
    "node_to_xla_file_map = {\n",
    "    0: \"/path/to/node_0/xla_dumps/module_xyz.jit_train_step.xxxxx_gpu_after_optimizations.txt\",\n",
    "    1: \"/path/to/node_1/xla_dumps/module_xyz.jit_train_step.xxxxx_gpu_after_optimizations.txt\",\n",
    "    # ... continue for all nodes\n",
    "}\n",
    "nccl_analyser = JaxNcclAnalyser(traces_dir, node_to_pb_file_mapping, world_size, node_to_xla_file_map=node_to_xla_file_map)\n",
    "```\n",
    "\n",
    "#### XLA Parser Capabilities\n",
    "\n",
    "The integrated `XLACollectiveParser` provides:\n",
    "- **Collective Detection**: Identifies all-reduce, all-gather, reduce-scatter, all-to-all, collective-permute operations\n",
    "- **Replica Group Parsing**: Handles complex formats including explicit groups `{{0,1,2,3},{4,5,6,7}}` and Replica group specifications using IotaTileAssignment format like`[4,8]<=[32]`\n",
    "- **Data Size Calculation**: Parses tensor shards dimensions and data types for precise bandwidth calculations\n",
    "\n",
    "### Processing Details\n",
    "\n",
    "The analyser will:\n",
    "- Process each node's protobuf trace file (`.xplane.pb`)\n",
    "- Filter for collective communication events\n",
    "- Parse XLA dump files to extract replica groups and data sizes\n",
    "- Build the foundational data structures needed for bandwidth analysis\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **`traces_dir`**: Directory containing trace files\n",
    "- **`node_to_pb_file_mapping`**: Mapping from nodes to protobuf file paths  \n",
    "- **`world_size`**: Total number of processes in the distributed setup\n",
    "- **`node_to_xla_file_map`**: Optional mapping to XLA files (**automatically built if None**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e04628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analyser - this loads all trace data\n",
    "print(\"Initializing JaxNcclAnalyser...\")\n",
    "nccl_analyser = JaxNcclAnalyser(traces_dir, node_to_pb_file_mapping, world_size)\n",
    "print(\"✓ Analyser initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3637a",
   "metadata": {},
   "source": [
    "## Step 3: Build the Analysis DataFrame\n",
    "\n",
    "Convert the loaded trace data into a structured long-format DataFrame for analysis. This core function processes all the trace events from each node and constructs a comprehensive table where **each row represents a single collective communication event on a specific GPU rank**.\n",
    "\n",
    "### What `build_df_long()` Does:\n",
    "\n",
    "**Data Transformation**: Processes the internal `node_to_trace_data` storage and flattens it into a structured table format\n",
    "\n",
    "**Event Enrichment**: For each collective communication event, extracts and combines:\n",
    "- **Timing information**: Timestamps (`ts`) and durations (`dur`) from trace events\n",
    "- **Process mapping**: Node ID, GPU rank, and pid for distributed setup tracking  \n",
    "- **Collective metadata**: Operation names (`collective_name`), HLO modules, and correlation IDs\n",
    "- **Querying XLA dump**: Extracts Replica groups and exchanged data during a collective event by lookup on parsed XLA dump dataframe\n",
    "\n",
    "#### Core Processing Steps\n",
    "\n",
    "The `build_df_long()` method performs these key operations:\n",
    "\n",
    "1. **Event Extraction**: Processes each collective communication event from all nodes\n",
    "2. **Row Construction**: For each event, creates a row with:\n",
    "   - Basic info: `node`, `gpu_rank`, `pid`, `ts` (timestamp), `dur` (duration)\n",
    "   - Collective metadata: `collective_name`, `hlo_module`, `correlation_id`\n",
    "   - XLA data: `replica_groups`, `data(bytes)` (looked up from parsed XLA dumps)\n",
    "\n",
    "3. **Indexing in group**: Groups events by `collective_name`, `pid`, and `node`, then assigns sequential indices based on timestamp order. \n",
    "4. **Collective ID Generation**: Creates unique identifiers using format `{collective_name}_{index_in_group}` \n",
    "\n",
    "#### Key Outputs\n",
    "\n",
    "- **Per-GPU Events**: Each row represents one collective event on a specific GPU rank\n",
    "- **Unique IDs**: `collective_id` enables grouping all GPUs participating in the same collective operation\n",
    "\n",
    "**Output**: A comprehensive DataFrame with columns including `node`, `gpu_rank`, `pid`, `ts`, `dur`, `collective_name`, `replica_groups`, `data(bytes)`, `collective_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a1da58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the long-format dataframe\n",
    "print(\"Building analysis dataframe...\")\n",
    "df = nccl_analyser.build_df_long()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e52760",
   "metadata": {},
   "source": [
    "## Step 4: Run Bandwidth Analysis\n",
    "\n",
    "Now we'll analyze the bandwidth performance of all collective operations. This calculates:\n",
    "- Bandwidth for each collective operation instance\n",
    "- Statistics across different slices (multiple communication events associated with each hlo-op)\n",
    "- Performance metrics per replica group\n",
    "\n",
    "### Core Processing in `analyze_all_collectives_from_df()`\n",
    "\n",
    "This method performs **comprehensive bandwidth analysis** of all collective communication operations found in the DataFrame. Here's the detailed breakdown of the core processing:\n",
    "\n",
    "#### 1. **Collective Discovery & Iteration**\n",
    "- **Extracts unique collective operations** from the DataFrame using `df[\"collective_name\"].unique()`\n",
    "- **Iterates through each collective type** (e.g., `all-reduce`, `all-gather`, `reduce-scatter`)\n",
    "\n",
    "#### 2. **Per-Collective Bandwidth Calculation**\n",
    "For each collective operation, the method calls `_calculate_collective_bandwidth_from_df()` which performs:\n",
    "\n",
    "##### **Slice Analysis** \n",
    "- **Groups by collective_id**: Each collective operation typically has multiple \"slices\" (communication events)\n",
    "- **Per-slice processing**: Analyzes bandwidth for each slice separately\n",
    "\n",
    "##### **Replica Group Bandwidth Calculation**\n",
    "- **Identifies participating GPUs**: Uses XLA dumps parsed replica groups \n",
    "- **Calculates timing**: Finds the fastest GPU in each replica group\n",
    "- **Algorithmic bandwidth formula**: `data_bytes / fastest_gpu_duration` \n",
    "- **Bus bandwidth calculation**: Applies collective-specific scaling factors to algorithimic bandwidth to calculate bus bandwidth\n",
    "\n",
    "\n",
    "##### **Example Processing Flow for a collective**:\n",
    "\n",
    "**All data here is placeholder to demonstrate structure**\n",
    "```\n",
    "Analyzing: xyz.28 (collective)\n",
    "Number of slices: xy\n",
    "\n",
    "  Slice: xyz.28_0\n",
    "    Available GPU ranks in data: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
    "      Analyzing group 0: [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "        Matching GPUs in data: [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "        Algorithmic Bandwidth: xyz.xy GB/s\n",
    "        Bus Bandwidth: abc.ab GB/s (scaler: 0.ab)\n",
    "        Fastest GPU: rank 3, duration: xyz.cd μs\n",
    "        Group data size: xyz bytes (0.ab GB)\n",
    "        Algorithmic bytes: wxyz (0.abc GB)\n",
    "        Replica group size: 8\n",
    "        ...\n",
    "\n",
    "    Slice summary:\n",
    "      Algorithmic BW: avg=xyz.xy GB/s\n",
    "      Bus BW: avg=xyz.xy GB/s      \n",
    "\n",
    "    ...\n",
    "\n",
    "Overall Results:\n",
    "  Average algorithmic bandwidth: xyz.xy GB/s\n",
    "  Average bus bandwidth: abc.ab GB/s\n",
    "```\n",
    "\n",
    "\n",
    "#### 3. **Statistical Aggregation**\n",
    "For each collective operation, computes:\n",
    "- **Per-slice bandwidths**: Array of bandwidth measurements across each slice\n",
    "- **Overall averages**: Mean bandwidth across all slices\n",
    "- **Data size information**: Bytes exchanged per operation \n",
    "\n",
    "\n",
    "#### 4. **Results Structure**\n",
    "Returns a comprehensive dictionary which can be used for further analysis as per user requirements:\n",
    "\n",
    "**All data here is placeholder to demonstrate structure**\n",
    "```python\n",
    "{\n",
    "  'xyz.28': {\n",
    "    'bandwidths': [100, 105, 110, 115, 120...],  # Per-slice algorithmic BW (GB/s)\n",
    "    'bus_bandwidths': [90, 95, 100, 105, 110 ...],  # Per-slice bus BW (GB/s)  \n",
    "    'avg_bandwidth': 100,                       # Overall algorithmic average (GB/s)\n",
    "    'avg_bus_bandwidth': 98,                   # Overall bus average (GB/s)\n",
    "    'num_slices': 35,                              # Number of slices\n",
    "    'data_size_bytes': 100000                   # 0.095 MB per group\n",
    "    'slice_info': [                                # Detailed per-slice data\n",
    "      {\n",
    "        'collective_id': 'xyz.28_0',\n",
    "        'group_bandwidths': [110, 115, 120, 125],  # Per replica group BW\n",
    "        'group_bus_bandwidths': [90, 95, 100, 105],  # Per replica group bus BW\n",
    "        'group_details': [\n",
    "          {\n",
    "            'group_idx': 0,\n",
    "            'gpu_group': [0, 1, 2, 3, 4, 5, 6, 7],\n",
    "            'bandwidth_gbps': 110,\n",
    "            'algorithmic_bandwidth_gbps': 110,\n",
    "            'bus_bandwidth_gbps': 90,\n",
    "            'bus_bandwidth_scaler': 0.875,\n",
    "            'duration_us': 1000,\n",
    "            'fastest_gpu_rank': 3,...        \n",
    "          }...\n",
    "        ],\n",
    "        'slice_avg_bandwidth': 117,\n",
    "        'slice_min_bandwidth': 100,\n",
    "        'slice_max_bandwidth': 120,\n",
    "        'num_groups': 4\n",
    "      }...\n",
    "    ]\n",
    "  }...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e64525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze bandwidth for all collective operations\n",
    "print(\"Running bandwidth analysis for all collective operations...\")\n",
    "bandwidth_results = nccl_analyser.analyze_all_collectives_from_df(df)\n",
    "print(f\"\\n✓ Analysis complete for {len(bandwidth_results)} collective operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07345119",
   "metadata": {},
   "source": [
    "## Step 5: Analyze by Collective Types\n",
    "\n",
    "Group the results by collective operation types (all-reduce, all-gather, etc.) to get aggregate performance statistics.\n",
    "\n",
    "### Core Processing Functions\n",
    "\n",
    "**`analyze_collective_types_from_df(bandwidth_results)`**:\n",
    "- **Groups collectives by type**: Categorizes individual collective operations (e.g., \"all-reduce-start.1\", \"all-reduce-start.2\") into collective types (\"all-reduce\")\n",
    "- **Aggregates bandwidth data**: Combines all bandwidth measurements for each collective type across all slices and operations\n",
    "- **Computes statistics**: Calculates mean for both algorithmic and bus bandwidth per collective type\n",
    "- **Returns**: Dictionary of collective type data and structured summary statistics\n",
    "\n",
    "**`display_summary_table(summary_stats)`**:\n",
    "- **Formats results**: Creates a formatted table displaying performance statistics for each collective type\n",
    "- **Shows key metrics**: Displays average algorithimic and bus bandwidths and operation counts\n",
    "- **Outputs**: A summary table with collective performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f8edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze performance by collective types\n",
    "collective_types_data, summary_stats = nccl_analyser.analyze_collective_types_from_df(bandwidth_results)\n",
    "# Display the summary table\n",
    "nccl_analyser.display_summary_table(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af07f080",
   "metadata": {},
   "source": [
    "## Step 6: Detailed Analysis of Specific Collective (OPTIONAL)\n",
    "\n",
    "Let's examine one collective operation in detail to understand the analysis methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the specific collective: xyz\n",
    "target_collective = \"xyz\" # Modify as per your case\n",
    "\n",
    "if bandwidth_results:\n",
    "    # Search for the target collective\n",
    "    collective_name = None\n",
    "    for name in bandwidth_results.keys():\n",
    "        if target_collective in name:\n",
    "            collective_name = name\n",
    "            break\n",
    "    \n",
    "    if collective_name:\n",
    "        result = bandwidth_results[collective_name]\n",
    "        \n",
    "        print(f\"=== DETAILED ANALYSIS: {collective_name} ===\")\n",
    "        print(f\"Average algorithmic bandwidth: {result['avg_bandwidth']:.2f} GB/s\")\n",
    "        print(f\"Average bus bandwidth: {result['avg_bus_bandwidth']:.2f} GB/s\")\n",
    "        print(f\"Number of slices: {result['num_slices']}\")\n",
    "\n",
    "        \n",
    "        if result['bandwidths']:\n",
    "            bandwidths = np.array(result['bandwidths'])\n",
    "            bus_bandwidths = np.array(result['bus_bandwidths'])\n",
    "            print(f\"Algorithmic bandwidth range: {bandwidths.min():.2f} - {bandwidths.max():.2f} GB/s\")\n",
    "            print(f\"Bus bandwidth range: {bus_bandwidths.min():.2f} - {bus_bandwidths.max():.2f} GB/s\")\n",
    "            print(f\"Algorithmic bandwidth std dev: {bandwidths.std():.2f} GB/s\")\n",
    "            print(f\"Bus bandwidth std dev: {bus_bandwidths.std():.2f} GB/s\")\n",
    "        \n",
    "        # Show detailed slice information\n",
    "        print(f\"\\n=== SLICE-BY-SLICE BREAKDOWN ===\")\n",
    "        for i, slice_info in enumerate(result['slice_info'][:3]):  # Show first 3 slices\n",
    "            print(f\"\\nSlice {i+1}: {slice_info['collective_id']}\")\n",
    "            print(f\"  Number of replica groups: {slice_info['num_groups']}\")\n",
    "            print(f\"  Slice average algorithmic bandwidth: {slice_info['slice_avg_bandwidth']:.2f} GB/s\")\n",
    "            print(f\"  Slice algorithmic bandwidth range: {slice_info['slice_min_bandwidth']:.2f} - {slice_info['slice_max_bandwidth']:.2f} GB/s\")\n",
    "            \n",
    "            # Show group details\n",
    "            for group_detail in slice_info['group_details'][:2]:  # Show first 2 groups per slice\n",
    "                print(f\"    Group {group_detail['group_idx']}: {group_detail['algorithmic_bandwidth_gbps']:.2f} GB/s (alg), {group_detail['bus_bandwidth_gbps']:.2f} GB/s (bus)\")\n",
    "                print(f\"      GPU group: {group_detail['gpu_group']}\")\n",
    "                print(f\"      Fastest GPU: rank {group_detail['fastest_gpu_rank']} ({group_detail['duration_us']:.0f} μs)\")\n",
    "    else:\n",
    "        print(f\"❌ Collective '{target_collective}' not found in bandwidth results\")\n",
    "        print(\"Available collectives:\")\n",
    "        for name in sorted(bandwidth_results.keys()):\n",
    "            print(f\"  - {name}\")\n",
    "else:\n",
    "    print(\"❌ No bandwidth results available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
