import pandas as pd
import numpy as np
from slodels import SLAIAzureOpenAI
import json
from pathlib import Path
import logging
from datetime import datetime
import argparse
import sys

class ComparisonResultAnalyzer:
    """
    Analyzes comparison_result.xlsx files generated by compare_perf_reports_pytorch.py
    Provides LLM-based insights on performance differences, regressions, and improvements
    """
    
    def __init__(self, api_key, model="gpt-4o", log_level=logging.INFO):
        self.client = SLAIAzureOpenAI(api_key=api_key) if api_key else None
        self.model = model
        self.comparison_data = {}
        
        # Setup logging
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'comparison_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def load_comparison_file(self, file_path):
        """Load comparison Excel file and all sheets"""
        print(f"\nLoading comparison file: {file_path}")
        print("="*80)
        
        try:
            xl_file = pd.ExcelFile(file_path)
            sheet_names = xl_file.sheet_names
            
            print(f"Found {len(sheet_names)} sheets:")
            for sheet_name in sheet_names:
                try:
                    df = pd.read_excel(file_path, sheet_name=sheet_name)
                    self.comparison_data[sheet_name] = df
                    print(f"  -> {sheet_name}: {df.shape[0]} rows x {df.shape[1]} columns")
                except Exception as e:
                    print(f"  -> Error loading sheet '{sheet_name}': {e}")
                    self.logger.error(f"Error loading sheet {sheet_name}: {e}")
            
            print(f"\nSuccessfully loaded {len(self.comparison_data)} sheets")
            return self.comparison_data
            
        except Exception as e:
            print(f"Error loading file {file_path}: {e}")
            self.logger.error(f"Error loading file {file_path}: {e}")
            return None
    
    def get_sheet_summary(self, sheet_name):
        """Generate comprehensive summary for a sheet"""
        if sheet_name not in self.comparison_data:
            return None
        
        df = self.comparison_data[sheet_name]
        
        # Identify column types
        key_cols = [col for col in df.columns if '::' not in col and not col.endswith(('_diff', '_pct'))]
        diff_cols = [col for col in df.columns if col.endswith('_diff')]
        pct_cols = [col for col in df.columns if col.endswith('_pct')]
        metric_cols = [col for col in df.columns if '::' in col]
        
        summary = {
            'sheet_name': sheet_name,
            'shape': df.shape,
            'key_columns': key_cols,
            'diff_columns': diff_cols,
            'pct_columns': pct_cols,
            'metric_columns': metric_cols,
            'total_rows': len(df),
            'non_null_rows': df.dropna(how='all').shape[0],
        }
        
        # Analyze diff and pct columns
        if diff_cols:
            summary['diff_stats'] = {}
            for col in diff_cols[:5]:  # Analyze first 5 diff columns
                summary['diff_stats'][col] = {
                    'mean': float(df[col].mean()) if pd.notna(df[col].mean()) else None,
                    'median': float(df[col].median()) if pd.notna(df[col].median()) else None,
                    'min': float(df[col].min()) if pd.notna(df[col].min()) else None,
                    'max': float(df[col].max()) if pd.notna(df[col].max()) else None,
                    'positive_count': int((df[col] > 0).sum()),
                    'negative_count': int((df[col] < 0).sum()),
                }
        
        if pct_cols:
            summary['pct_stats'] = {}
            for col in pct_cols[:5]:  # Analyze first 5 pct columns
                summary['pct_stats'][col] = {
                    'mean': float(df[col].mean()) if pd.notna(df[col].mean()) else None,
                    'median': float(df[col].median()) if pd.notna(df[col].median()) else None,
                    'improvements': int((df[col] < 0).sum()),  # negative % is improvement
                    'regressions': int((df[col] > 0).sum()),   # positive % is regression
                }
        
        # Sample data
        summary['sample_data'] = df.head(3).to_dict('records')
        
        return summary
    
    def analyze_ops_summary(self, sheet_name='ops_summary'):
        """Analyze ops_summary sheet specifically"""
        if sheet_name not in self.comparison_data:
            print(f"Sheet '{sheet_name}' not found")
            return None
        
        df = self.comparison_data[sheet_name]
        
        print(f"\n{'='*80}")
        print(f"ANALYZING: {sheet_name}")
        print(f"{'='*80}\n")
        
        # Find time diff and pct columns
        time_diff_cols = [col for col in df.columns if 'kernel_time' in col.lower() and col.endswith('_diff')]
        time_pct_cols = [col for col in df.columns if 'kernel_time' in col.lower() and col.endswith('_pct')]
        
        analysis = {
            'total_ops': len(df),
            'analyzed_metrics': [],
        }
        
        # Analyze each time comparison
        for diff_col, pct_col in zip(time_diff_cols, time_pct_cols):
            variant_name = diff_col.split('__')[1].replace('_diff', '')
            
            improvements = df[df[pct_col] < 0].copy()
            regressions = df[df[pct_col] > 0].copy()
            
            metric_analysis = {
                'variant': variant_name,
                'total_improvements': len(improvements),
                'total_regressions': len(regressions),
                'neutral': len(df) - len(improvements) - len(regressions),
            }
            
            if len(improvements) > 0:
                improvements_sorted = improvements.sort_values(pct_col)
                metric_analysis['top_improvements'] = improvements_sorted.head(10)[['name', diff_col, pct_col]].to_dict('records')
                metric_analysis['avg_improvement_pct'] = float(improvements[pct_col].mean())
            
            if len(regressions) > 0:
                regressions_sorted = regressions.sort_values(pct_col, ascending=False)
                metric_analysis['top_regressions'] = regressions_sorted.head(10)[['name', diff_col, pct_col]].to_dict('records')
                metric_analysis['avg_regression_pct'] = float(regressions[pct_col].mean())
            
            analysis['analyzed_metrics'].append(metric_analysis)
        
        return analysis
    
    def analyze_intersect_sheet(self, sheet_pattern='intersect'):
        """Analyze intersect sheets (ops present in both baseline and variant)"""
        intersect_sheets = [s for s in self.comparison_data.keys() if sheet_pattern in s.lower()]
        
        if not intersect_sheets:
            print(f"No sheets matching pattern '{sheet_pattern}' found")
            return None
        
        all_analysis = {}
        
        for sheet_name in intersect_sheets:
            df = self.comparison_data[sheet_name]
            
            print(f"\n{'='*80}")
            print(f"ANALYZING: {sheet_name}")
            print(f"{'='*80}\n")
            
            # Find diff and pct columns
            diff_cols = [col for col in df.columns if col.endswith('_diff')]
            pct_cols = [col for col in df.columns if col.endswith('_pct')]
            
            analysis = {
                'sheet_name': sheet_name,
                'total_rows': len(df),
                'metrics': []
            }
            
            for diff_col in diff_cols[:3]:  # Analyze top 3 metrics
                base_metric = diff_col.replace('__', '::').rsplit('_', 2)[0]
                pct_col = diff_col.replace('_diff', '_pct')
                
                if pct_col in df.columns:
                    improvements = df[df[pct_col] < 0]
                    regressions = df[df[pct_col] > 0]
                    
                    metric_info = {
                        'metric': base_metric,
                        'improvements': len(improvements),
                        'regressions': len(regressions),
                        'neutral': len(df) - len(improvements) - len(regressions),
                    }
                    
                    if len(improvements) > 0:
                        metric_info['avg_improvement_pct'] = float(improvements[pct_col].mean())
                        metric_info['max_improvement_pct'] = float(improvements[pct_col].min())
                    
                    if len(regressions) > 0:
                        metric_info['avg_regression_pct'] = float(regressions[pct_col].mean())
                        metric_info['max_regression_pct'] = float(regressions[pct_col].max())
                    
                    analysis['metrics'].append(metric_info)
            
            all_analysis[sheet_name] = analysis
        
        return all_analysis
    
    def analyze_baseline_only_sheets(self):
        """Analyze operations that only exist in baseline (removed in variant)"""
        baseline_sheets = [s for s in self.comparison_data.keys() if 'only_baseline' in s.lower()]
        
        if not baseline_sheets:
            print("No 'only_baseline' sheets found")
            return None
        
        analysis = {}
        
        for sheet_name in baseline_sheets:
            df = self.comparison_data[sheet_name]
            
            print(f"\n{'='*80}")
            print(f"ANALYZING: {sheet_name}")
            print(f"{'='*80}\n")
            
            print(f"Operations only in baseline: {len(df)}")
            
            # Find baseline metric columns
            baseline_cols = [col for col in df.columns if '::' in col]
            
            sheet_analysis = {
                'sheet_name': sheet_name,
                'total_ops_removed': len(df),
                'columns': baseline_cols,
            }
            
            # Try to find time-related columns
            time_cols = [col for col in baseline_cols if 'time' in col.lower() or 'kernel' in col.lower()]
            if time_cols and len(df) > 0:
                time_col = time_cols[0]
                df_sorted = df.sort_values(time_col, ascending=False)
                sheet_analysis['top_removed_ops'] = df_sorted.head(10)[['name'] + [time_col]].to_dict('records')
                sheet_analysis['total_time_removed'] = float(df[time_col].sum()) if pd.notna(df[time_col].sum()) else None
            
            analysis[sheet_name] = sheet_analysis
        
        return analysis
    
    def analyze_variant_only_sheets(self):
        """Analyze operations that only exist in variant (new in variant)"""
        variant_sheets = [s for s in self.comparison_data.keys() if 'only_variant' in s.lower()]
        
        if not variant_sheets:
            print("No 'only_variant' sheets found")
            return None
        
        analysis = {}
        
        for sheet_name in variant_sheets:
            df = self.comparison_data[sheet_name]
            
            print(f"\n{'='*80}")
            print(f"ANALYZING: {sheet_name}")
            print(f"{'='*80}\n")
            
            print(f"New operations in variant: {len(df)}")
            
            # Find variant metric columns
            variant_cols = [col for col in df.columns if '::' in col]
            
            sheet_analysis = {
                'sheet_name': sheet_name,
                'total_new_ops': len(df),
                'columns': variant_cols,
            }
            
            # Try to find time-related columns
            time_cols = [col for col in variant_cols if 'time' in col.lower() or 'kernel' in col.lower()]
            if time_cols and len(df) > 0:
                time_col = time_cols[0]
                df_sorted = df.sort_values(time_col, ascending=False)
                sheet_analysis['top_new_ops'] = df_sorted.head(10)[['name'] + [time_col]].to_dict('records')
                sheet_analysis['total_time_added'] = float(df[time_col].sum()) if pd.notna(df[time_col].sum()) else None
            
            analysis[sheet_name] = sheet_analysis
        
        return analysis
    
    def create_ai_prompt_for_comparison(self, analysis_data, analysis_type="overall"):
        """Create AI prompts for different analysis types"""
        
        prompts = {
            "overall": f"""
You are a GPU performance analysis expert. Analyze this comparison data between baseline and variant performance reports.

Analysis Data:
{json.dumps(analysis_data, indent=2, default=str)}

Please provide:
1. Executive Summary: High-level performance comparison verdict (better/worse/mixed)
2. Key Performance Insights: Major improvements and regressions
3. Critical Regressions: Operations with significant slowdowns that need attention
4. Top Improvements: Operations with best performance gains
5. Overall Recommendations: Actionable next steps for optimization

Focus on practical insights and prioritize issues by impact.
""",
            
            "ops_summary": f"""
You are analyzing operation-level performance comparison data.

Data:
{json.dumps(analysis_data, indent=2, default=str)}

Provide:
1. Operation Performance Overview: Which operation categories improved/regressed?
2. Hotspot Analysis: Which operations consume most time and how did they change?
3. Pattern Detection: Any patterns in improvements/regressions (e.g., all GEMM ops improved)?
4. Risk Assessment: Which regressions are most concerning?
5. Optimization Priorities: What should be optimized next?
""",
            
            "intersect_analysis": f"""
You are analyzing operations present in both baseline and variant reports.

Data:
{json.dumps(analysis_data, indent=2, default=str)}

Analyze:
1. Performance Deltas: Overall trend (improving or regressing)?
2. Consistency: Are changes consistent or scattered?
3. Outliers: Any operations with extreme changes?
4. Metric Correlations: Do different metrics show similar trends?
5. Root Cause Hypotheses: What might explain the observed changes?
""",
            
            "baseline_only": f"""
You are analyzing operations that exist in baseline but not in variant (removed operations).

Data:
{json.dumps(analysis_data, indent=2, default=str)}

Evaluate:
1. Removal Impact: Is removing these operations good or bad?
2. Potential Reasons: Why might these operations be removed?
3. Performance Implications: What's the cumulative time saved/lost?
4. Functional Risk: Could removal cause correctness issues?
5. Recommendations: Should any removals be reconsidered?
""",
            
            "variant_only": f"""
You are analyzing new operations that appear only in the variant report.

Data:
{json.dumps(analysis_data, indent=2, default=str)}

Assess:
1. Addition Impact: Are these additions beneficial or overhead?
2. Potential Reasons: Why might these operations be added?
3. Performance Cost: What's the cumulative time cost of additions?
4. Optimization Opportunity: Can new operations be optimized?
5. Trade-off Analysis: Are additions worth their cost?
""",
            
            "roofline_analysis": f"""
You are analyzing roofline model data (GEMM, SDPA, CONV, elementwise operations).

Data:
{json.dumps(analysis_data, indent=2, default=str)}

Provide:
1. Compute Efficiency: How did TFLOPS/s change?
2. Memory Efficiency: How did bandwidth utilization change?
3. Roofline Position: Are operations compute-bound or memory-bound?
4. Optimization Opportunities: Which operations are under-utilizing hardware?
5. Performance Recommendations: Specific optimizations for each operation type
""",
        }
        
        return prompts.get(analysis_type, prompts["overall"])
    
    def ai_analyze(self, prompt, max_tokens=3000):
        """Send prompt to AI and get analysis"""
        if not self.client:
            return "AI analysis disabled (no API key provided)"
        
        try:
            print("\nQuerying AI for analysis...")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are a GPU performance optimization expert specializing in PyTorch and CUDA operations."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            error_msg = f"Error in AI analysis: {e}"
            self.logger.error(error_msg)
            return error_msg
    
    def generate_comprehensive_report(self, output_file=None):
        """Generate a comprehensive analysis report"""
        
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"comparison_analysis_report_{timestamp}.txt"
        
        print(f"\n{'='*80}")
        print("GENERATING COMPREHENSIVE ANALYSIS REPORT")
        print(f"{'='*80}\n")
        
        with open(output_file, 'w') as f:
            # Header
            f.write("="*80 + "\n")
            f.write("GPU PERFORMANCE COMPARISON ANALYSIS REPORT\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*80 + "\n\n")
            
            # 1. Ops Summary Analysis
            if 'ops_summary' in self.comparison_data:
                f.write("\n" + "="*80 + "\n")
                f.write("SECTION 1: OPERATIONS SUMMARY ANALYSIS\n")
                f.write("="*80 + "\n\n")
                
                ops_analysis = self.analyze_ops_summary()
                f.write(json.dumps(ops_analysis, indent=2, default=str))
                f.write("\n\n")
                
                if self.client:
                    prompt = self.create_ai_prompt_for_comparison(ops_analysis, "ops_summary")
                    ai_response = self.ai_analyze(prompt)
                    f.write("\nAI INSIGHTS:\n")
                    f.write("-"*80 + "\n")
                    f.write(ai_response)
                    f.write("\n\n")
            
            # 2. Intersect Analysis
            intersect_analysis = self.analyze_intersect_sheet()
            if intersect_analysis:
                f.write("\n" + "="*80 + "\n")
                f.write("SECTION 2: COMMON OPERATIONS ANALYSIS (INTERSECT)\n")
                f.write("="*80 + "\n\n")
                
                f.write(json.dumps(intersect_analysis, indent=2, default=str))
                f.write("\n\n")
                
                if self.client:
                    prompt = self.create_ai_prompt_for_comparison(intersect_analysis, "intersect_analysis")
                    ai_response = self.ai_analyze(prompt)
                    f.write("\nAI INSIGHTS:\n")
                    f.write("-"*80 + "\n")
                    f.write(ai_response)
                    f.write("\n\n")
            
            # 3. Baseline Only Analysis
            baseline_analysis = self.analyze_baseline_only_sheets()
            if baseline_analysis:
                f.write("\n" + "="*80 + "\n")
                f.write("SECTION 3: REMOVED OPERATIONS ANALYSIS (BASELINE ONLY)\n")
                f.write("="*80 + "\n\n")
                
                f.write(json.dumps(baseline_analysis, indent=2, default=str))
                f.write("\n\n")
                
                if self.client:
                    prompt = self.create_ai_prompt_for_comparison(baseline_analysis, "baseline_only")
                    ai_response = self.ai_analyze(prompt)
                    f.write("\nAI INSIGHTS:\n")
                    f.write("-"*80 + "\n")
                    f.write(ai_response)
                    f.write("\n\n")
            
            # 4. Variant Only Analysis
            variant_analysis = self.analyze_variant_only_sheets()
            if variant_analysis:
                f.write("\n" + "="*80 + "\n")
                f.write("SECTION 4: NEW OPERATIONS ANALYSIS (VARIANT ONLY)\n")
                f.write("="*80 + "\n\n")
                
                f.write(json.dumps(variant_analysis, indent=2, default=str))
                f.write("\n\n")
                
                if self.client:
                    prompt = self.create_ai_prompt_for_comparison(variant_analysis, "variant_only")
                    ai_response = self.ai_analyze(prompt)
                    f.write("\nAI INSIGHTS:\n")
                    f.write("-"*80 + "\n")
                    f.write(ai_response)
                    f.write("\n\n")
            
            # 5. Roofline Analysis (if exists)
            roofline_sheets = [s for s in self.comparison_data.keys() 
                             if any(x in s.lower() for x in ['gemm', 'sdpa', 'conv', 'eltwise'])]
            
            if roofline_sheets:
                f.write("\n" + "="*80 + "\n")
                f.write("SECTION 5: ROOFLINE MODEL ANALYSIS\n")
                f.write("="*80 + "\n\n")
                
                for sheet in roofline_sheets[:5]:  # Analyze first 5 roofline sheets
                    f.write(f"\n--- {sheet} ---\n")
                    summary = self.get_sheet_summary(sheet)
                    f.write(json.dumps(summary, indent=2, default=str))
                    f.write("\n")
                
                if self.client:
                    roofline_data = {sheet: self.get_sheet_summary(sheet) for sheet in roofline_sheets[:3]}
                    prompt = self.create_ai_prompt_for_comparison(roofline_data, "roofline_analysis")
                    ai_response = self.ai_analyze(prompt, max_tokens=4000)
                    f.write("\nAI INSIGHTS:\n")
                    f.write("-"*80 + "\n")
                    f.write(ai_response)
                    f.write("\n\n")
            
            # 6. Overall Summary
            f.write("\n" + "="*80 + "\n")
            f.write("SECTION 6: OVERALL SUMMARY AND RECOMMENDATIONS\n")
            f.write("="*80 + "\n\n")
            
            if self.client:
                # Combine all analyses for overall summary
                overall_data = {
                    'ops_summary': ops_analysis if 'ops_summary' in self.comparison_data else None,
                    'intersect_sheets': intersect_analysis,
                    'baseline_only_sheets': baseline_analysis,
                    'variant_only_sheets': variant_analysis,
                    'total_sheets_analyzed': len(self.comparison_data),
                }
                
                prompt = self.create_ai_prompt_for_comparison(overall_data, "overall")
                ai_response = self.ai_analyze(prompt, max_tokens=4000)
                f.write(ai_response)
                f.write("\n\n")
            
            f.write("\n" + "="*80 + "\n")
            f.write("END OF REPORT\n")
            f.write("="*80 + "\n")
        
        print(f"\nReport saved to: {output_file}")
        return output_file
    
    def interactive_analysis(self):
        """Interactive mode for targeted analysis"""
        if not self.comparison_data:
            print("No data loaded. Please load a comparison file first.")
            return
        
        while True:
            print("\n" + "="*80)
            print("INTERACTIVE ANALYSIS MENU")
            print("="*80)
            print("1. List all sheets")
            print("2. Analyze specific sheet")
            print("3. Analyze ops_summary")
            print("4. Analyze all intersect sheets")
            print("5. Analyze baseline-only sheets")
            print("6. Analyze variant-only sheets")
            print("7. Generate comprehensive report")
            print("8. Exit")
            
            choice = input("\nEnter your choice (1-8): ").strip()
            
            if choice == '1':
                print("\nAvailable sheets:")
                for i, sheet in enumerate(self.comparison_data.keys(), 1):
                    print(f"  {i}. {sheet} ({self.comparison_data[sheet].shape})")
            
            elif choice == '2':
                sheet_name = input("Enter sheet name: ").strip()
                if sheet_name in self.comparison_data:
                    summary = self.get_sheet_summary(sheet_name)
                    print(json.dumps(summary, indent=2, default=str))
                    
                    if self.client and input("\nGet AI analysis? (y/n): ").lower() == 'y':
                        prompt = self.create_ai_prompt_for_comparison(summary, "overall")
                        print(self.ai_analyze(prompt))
                else:
                    print(f"Sheet '{sheet_name}' not found")
            
            elif choice == '3':
                analysis = self.analyze_ops_summary()
                if analysis:
                    print(json.dumps(analysis, indent=2, default=str))
                    if self.client and input("\nGet AI analysis? (y/n): ").lower() == 'y':
                        prompt = self.create_ai_prompt_for_comparison(analysis, "ops_summary")
                        print(self.ai_analyze(prompt))
            
            elif choice == '4':
                analysis = self.analyze_intersect_sheet()
                if analysis:
                    print(json.dumps(analysis, indent=2, default=str))
                    if self.client and input("\nGet AI analysis? (y/n): ").lower() == 'y':
                        prompt = self.create_ai_prompt_for_comparison(analysis, "intersect_analysis")
                        print(self.ai_analyze(prompt))
            
            elif choice == '5':
                analysis = self.analyze_baseline_only_sheets()
                if analysis:
                    print(json.dumps(analysis, indent=2, default=str))
                    if self.client and input("\nGet AI analysis? (y/n): ").lower() == 'y':
                        prompt = self.create_ai_prompt_for_comparison(analysis, "baseline_only")
                        print(self.ai_analyze(prompt))
            
            elif choice == '6':
                analysis = self.analyze_variant_only_sheets()
                if analysis:
                    print(json.dumps(analysis, indent=2, default=str))
                    if self.client and input("\nGet AI analysis? (y/n): ").lower() == 'y':
                        prompt = self.create_ai_prompt_for_comparison(analysis, "variant_only")
                        print(self.ai_analyze(prompt))
            
            elif choice == '7':
                output_file = input("Enter output filename (press Enter for default): ").strip()
                self.generate_comprehensive_report(output_file if output_file else None)
            
            elif choice == '8':
                print("Exiting...")
                break
            
            else:
                print("Invalid choice. Please try again.")


def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description='Comparison Result Analyzer - AI-powered analysis of GPU performance comparison reports',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Generate comprehensive report
  python comparison-analysis.py comparison_result.xlsx --report
  
  # Interactive mode
  python comparison-analysis.py comparison_result.xlsx --interactive
  
  # Analyze specific aspects
  python comparison-analysis.py comparison_result.xlsx --ops-summary --intersect
  
  # Without AI (basic statistics only)
  python comparison-analysis.py comparison_result.xlsx --no-ai --report
        """
    )
    
    parser.add_argument(
        'comparison_file',
        type=str,
        help='Path to comparison_result.xlsx file'
    )
    
    parser.add_argument(
        '--api-key',
        type=str,
        default='',
        help='Azure OpenAI API key'
    )
    
    parser.add_argument(
        '--model',
        type=str,
        default='gpt-4o',
        help='AI model to use (default: gpt-4o)'
    )
    
    parser.add_argument(
        '--report',
        action='store_true',
        help='Generate comprehensive analysis report'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        help='Output report filename'
    )
    
    parser.add_argument(
        '--interactive',
        action='store_true',
        help='Run in interactive mode'
    )
    
    parser.add_argument(
        '--ops-summary',
        action='store_true',
        help='Analyze ops_summary sheet only'
    )
    
    parser.add_argument(
        '--intersect',
        action='store_true',
        help='Analyze intersect sheets only'
    )
    
    parser.add_argument(
        '--baseline-only',
        action='store_true',
        help='Analyze baseline-only sheets only'
    )
    
    parser.add_argument(
        '--variant-only',
        action='store_true',
        help='Analyze variant-only sheets only'
    )
    
    parser.add_argument(
        '--no-ai',
        action='store_true',
        help='Skip AI analysis (statistics only)'
    )
    
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='Enable verbose logging'
    )
    
    return parser.parse_args()


def main():
    args = parse_arguments()
    
    # Initialize analyzer
    log_level = logging.DEBUG if args.verbose else logging.INFO
    api_key = None if args.no_ai else args.api_key
    
    analyzer = ComparisonResultAnalyzer(
        api_key=api_key,
        model=args.model,
        log_level=log_level
    )
    
    # Load comparison file
    if not analyzer.load_comparison_file(args.comparison_file):
        print("Failed to load comparison file")
        sys.exit(1)
    
    # Execute requested analysis
    if args.interactive:
        analyzer.interactive_analysis()
    
    elif args.report:
        analyzer.generate_comprehensive_report(args.output)
    
    elif args.ops_summary:
        analysis = analyzer.analyze_ops_summary()
        print(json.dumps(analysis, indent=2, default=str))
        if not args.no_ai:
            prompt = analyzer.create_ai_prompt_for_comparison(analysis, "ops_summary")
            print("\nAI ANALYSIS:")
            print("="*80)
            print(analyzer.ai_analyze(prompt))
    
    elif args.intersect:
        analysis = analyzer.analyze_intersect_sheet()
        print(json.dumps(analysis, indent=2, default=str))
        if not args.no_ai:
            prompt = analyzer.create_ai_prompt_for_comparison(analysis, "intersect_analysis")
            print("\nAI ANALYSIS:")
            print("="*80)
            print(analyzer.ai_analyze(prompt))
    
    elif args.baseline_only:
        analysis = analyzer.analyze_baseline_only_sheets()
        print(json.dumps(analysis, indent=2, default=str))
        if not args.no_ai:
            prompt = analyzer.create_ai_prompt_for_comparison(analysis, "baseline_only")
            print("\nAI ANALYSIS:")
            print("="*80)
            print(analyzer.ai_analyze(prompt))
    
    elif args.variant_only:
        analysis = analyzer.analyze_variant_only_sheets()
        print(json.dumps(analysis, indent=2, default=str))
        if not args.no_ai:
            prompt = analyzer.create_ai_prompt_for_comparison(analysis, "variant_only")
            print("\nAI ANALYSIS:")
            print("="*80)
            print(analyzer.ai_analyze(prompt))
    
    else:
        # Default: show available sheets and prompt for action
        print("\nNo specific analysis requested. Available options:")
        print("  --report          Generate comprehensive report")
        print("  --interactive     Interactive analysis mode")
        print("  --ops-summary     Analyze operations summary")
        print("  --intersect       Analyze common operations")
        print("  --baseline-only   Analyze removed operations")
        print("  --variant-only    Analyze new operations")
        print("\nRun with --help for more details")


if __name__ == "__main__":
    main()

