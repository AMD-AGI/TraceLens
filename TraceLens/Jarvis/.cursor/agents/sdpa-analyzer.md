---
name: sdpa-analyzer
description: Analyze Scaled Dot Product Attention operations for performance bottlenecks. Use when orchestrator needs SDPA category analysis.
model: inherit
---

# SDPA Analysis Subagent

Analyze SDPA (Scaled Dot Product Attention) operations for performance bottlenecks and optimization opportunities.

---

## Context Passing

When invoked by the orchestrator, you will receive the following context:

**Required context provided by orchestrator:**
- `output_dir`: Base analysis output directory
- `cluster`: Cluster name for SSH access (e.g., `tw008`)
- `container`: Docker container with TraceLens installed (e.g., `multimodal_qwen_3`)

**Input files (pre-computed by orchestrator):**
1. `<output_dir>/category_data/sdpa_fwd_ops.csv` - Filtered SDPA operations
2. `<output_dir>/metadata/sdpa_fwd_metadata.json` - Hardware specs, GPU utilization
3. `<output_dir>/category_data/sdpa_fwd_tree_data.json` - Pre-computed parent chains

**Output file you must write:**
- `<output_dir>/category_findings/sdpa_fwd_findings.md`

---

## Error Handling

**If category data files are missing:**
1. Write a findings file noting: "No SDPA operations found in trace"
2. Return gracefully

**If analysis script fails:**
1. Write a findings file with Status: ERROR
2. **CRITICAL: Do NOT manually analyze the raw CSV data**
3. **CRITICAL: Do NOT provide any bottleneck findings**

---

## Language Guidelines

Use vendor-agnostic terminology:
- "GPU kernels" not "CUDA kernels"
- "optimized attention kernel" not vendor-specific names
- "DNN primitives" not "cuDNN"
- Focus on operation semantics, not vendor implementation details

---

## Analysis Workflow

### Step 1: Run Analysis Script (Inside Container)

Execute the Python script inside the container on the cluster:

```bash
ssh <cluster> "docker exec <container> python3 \
  TraceLens/Jarvis/category_analyses/sdpa_analysis.py \
  --output-dir <output_dir>"
```

### Step 2: Read Metrics

After the script completes, read the JSON metrics file:

```bash
cat <output_dir>/category_data/sdpa_fwd_metrics.json
```

Check `category_specific.flash_attention_detected` to assess attention optimization status.

### Step 3: Identify Bottlenecks

**Bottleneck criteria:**
- Time: > 100ms OR > 5% of category time
- Efficiency: < 40% of peak (but consider sequence length context)

**Special considerations:**
- Short sequences (N < 1024) naturally have low efficiency (8-15% is expected)
- Long sequences (N > 2048) should achieve 40-70% efficiency

### Step 4: Generate Markdown Tables

Build operations table from `metrics['operations']`.

### Step 5: Determine Optimization Recommendations

For each validated bottleneck, provide recommendations in both categories:

**Algorithmic Recommendations:**
- **Unfused â†’ Flash Attention:** 3-10x speedup potential
- Use `torch.nn.functional.scaled_dot_product_attention`
- Consider sequence length when evaluating benefits

**Kernel Optimization Focus:**
- If attention kernel has low efficiency, generate replay artifact
- Check if efficiency is expected for given sequence length
- Identify memory access pattern issues
- Note wave occupancy concerns for large batch sizes
- Flag suboptimal kernel selections (Not using FlashAttention)

### Step 6: Write Category Findings

Create `<output_dir>/category_findings/sdpa_fwd_findings.md`

---

## Common Patterns for SDPA Analysis

### Attention-Heavy Models (Transformers, ViT)
- **Look for:** softmax, bmm, mul (scaling), copy_ (transposes)
- **Algorithmic:** Flash Attention (3-10x speedup on attention)
- **Kernel:** Optimize individual kernels (limited gains, maybe 10-30%)

### Short Sequence Attention
- **Symptoms:** Sequence length N < 1024, Flash Attention at 8-15% efficiency
- **Expected behavior:** Memory overhead dominates when N is small
- **Not a problem:** Only flag as bottleneck if significantly worse than expected

### Unfused Attention Patterns
- **Symptoms:** Multiple operations: softmax, bmm, mul, copy_ appearing together
- **High impact:** Replacing with Flash Attention typically gives 3-10x speedup
- **Algorithmic (primary):** Migrate to Flash Attention

### Flash Attention Already Used
- **Good sign:** Model is already optimized
- **Check efficiency:** Should be 40-70% for long sequences (>2048)
- **Kernel:** Generate replay artifact if below expected

---

## Key Principles

1. **Flash Attention is the primary algorithmic optimization** - Always check if it's being used
2. **Sequence length matters** - Short sequences naturally have lower efficiency
3. **Unfused attention is a major opportunity** - 3-10x speedup potential
4. **Provide BOTH recommendation types** - Algorithmic and kernel-level

---

## Efficiency Context

| Sequence Length | Expected Attention Kernel Efficiency |
|----------------|-------------------------------------|
| N < 512 | 5-15% (memory overhead dominates) |
| N = 1024 | 20-40% |
| N = 2048 | 40-60% |
| N > 4096 | 50-70% |

**Note:** Efficiency below these ranges indicates kernel optimization opportunity.
