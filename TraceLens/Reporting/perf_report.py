import argparse, os, sys
import json
from pathlib import Path
from collections import defaultdict, Counter

from TraceLens import TreePerfAnalyzer
from TraceLens.PerfModel import dict_cat2names
from TraceLens.TreePerf import TreePerfAnalyzer, JaxTreePerfAnalyser, JaxAnalyses
from TraceLens.Reporting.reporting_utils import export_data_df
from TraceLens.util import TraceEventUtils

def perf_analysis(profile_path: str, arch = None, agg_metrics = ['mean', 'median', 'std', 'min', 'max'], *args, **kwargs) -> dict:
    """
    Generates a performance report for Pytorch analysis from a given profile trace.json file.
    This function processes GPU event statistics and GEMM (General Matrix Multiply) performance data
    from the specified profile file, and exports the results into a dictionary of Dataframes.
    Args:
        profile_path (str): Path to the input XPlane profile protobuf file.
        # *args, **kwargs are passed to the TreePerfAnalyzer constructor.
    Outputs:
        Writes multiple DataFrames containing GPU event statistics and GEMM performance data
        to a dictionary of Dataframes with appropriate suffixes.
    """
    # Get input trace type
    if profile_path.endswith('.pt.trace.json'):
        perf_analyzer = TreePerfAnalyzer.from_file(profile_filepath=profile_path, arch=arch)
    elif profile_path.endswith('.xplane.pb'):
        perf_analyzer = JaxTreePerfAnalyser.from_file(profile_filepath=profile_path)
    else:
        print('Unsupported trace file format.')
        pass
    dict_dfs = {}

    # Generate base DataFrames 
    df_gpu_timeline = perf_analyzer.get_df_gpu_timeline() 
    df_kernel_launchers = perf_analyzer.get_df_kernel_launchers(include_kernel_details=True)
    df_kernel_launchers_summary = perf_analyzer.get_df_kernel_launchers_summary(df_kernel_launchers)
    df_kernel_launchers_summary_by_category = perf_analyzer.get_df_kernel_launchers_summary_by_category(df_kernel_launchers)
    df_kernel_launchers_unique_args = perf_analyzer.get_df_kernel_launchers_unique_args(df_kernel_launchers, 
                                                                                    agg_metrics=agg_metrics, 
                                                                                    include_pct=True)

    # Store base DataFrames
    dict_dfs['gpu_timeline']= df_gpu_timeline
    dict_dfs['kernel_launchers']= df_kernel_launchers
    dict_dfs['kernel_launchers_summary']= df_kernel_launchers_summary
    dict_dfs['kernel_launchers_summary_by_category']= df_kernel_launchers_summary_by_category 
    dict_dfs['kernel_launchers_unique_args']= df_kernel_launchers_unique_args
    return dict_dfs 

def perf_pytorch(profile_path: str, arch = None, agg_metrics = ['mean', 'median', 'std', 'min', 'max'], *args, **kwargs) -> dict:
    perf_analyzer = TreePerfAnalyzer.from_file(profile_filepath=profile_path, arch=arch)
    dict_dfs = {}

    # Generate & store op-specific DataFrames
    for op_cat, op_names in dict_cat2names.items():
        # Filter events belonging to the current category
        op_events = [event for event in perf_analyzer.tree.events if event['name'] in op_names]
        if op_cat in ['GEMM', 'UnaryElementwise', 'BinaryElementwise']: 
            # For GEMM: create a single table that covers both fwd and bwd.
            df_ops = perf_analyzer.build_df_perf_metrics(op_events, bwd=False, include_kernel_details=True, include_args=True)
            df_ops = perf_analyzer.summarize_df_perf_metrics(df_ops, agg_metrics)
            dict_dfs[f"op_{op_cat}"] = df_ops
        else:
            # For FLASH_ATTN and CONV: create separate tables for forward and backward passes.
            df_ops_fwd = perf_analyzer.build_df_perf_metrics(op_events, bwd=False, include_kernel_details=True, include_args=True)
            df_ops_fwd = perf_analyzer.summarize_df_perf_metrics(df_ops_fwd, agg_metrics)
            df_ops_bwd = perf_analyzer.build_df_perf_metrics(op_events, bwd=True, include_kernel_details=True, include_args=True)
            df_ops_bwd = perf_analyzer.summarize_df_perf_metrics(df_ops_bwd, agg_metrics)
            dict_dfs[f"op_{op_cat}_fwd"] = df_ops_fwd
            dict_dfs[f"op_{op_cat}_bwd"] = df_ops_bwd

    return dict_dfs

def perf_jax(profile_path: str, agg_metrics = ['mean', 'median', 'std', 'min', 'max'], *args, **kwargs) -> dict:
    """
    Generates a performance report for JAX analysis from a given XPlane profile protobuf file.
    This function processes GPU event statistics and GEMM (General Matrix Multiply) performance data
    from the specified profile file, and exports the results into a dictionary of Dataframes.

    It summarizes GPU events by calculating averages, categorizing events,
    and grouping XLA events by their base names. It also computes the overlapped communication
    time and appends it to the averages DataFrame.

    Args:
        profile_path (str): Path to the input XPlane profile protobuf file generated by JAX.
        num_cus (int, optional): Number of compute units (CUs) for the GPU architecture. Defaults to 304.

    Returns:
        dict:
            - df_gpu_events_averages (pd.DataFrame): DataFrame containing average times and percentages for various GPU event types, including overlapped communication.
            - df_gpu_events_categorized_mean (pd.DataFrame): DataFrame with categorized GPU event statistics, indexed and renamed for clarity.
            - df_xla_grouped (pd.DataFrame): DataFrame of XLA events grouped by base name, sorted by percentage of total time.
            - df_gemms_detailed(pd.DataFrame): DataFrame of GEMMs
    """
    perf_analyzer = JaxTreePerfAnalyser.from_file(profile_filepath=profile_path)
    dict_dfs = {}

    # Debug event filters
    if 1:
        kernel_events =  [event for event in perf_analyzer.tree.events if event['cat']=='kernel']
        for op_cat in ['GEMM', 'Conv', 'TE', 'FA FWD', 'FA BWD']: # TraceEventUtils.JaxOpKeys.ClassCategories.keys()
            op_events = [event for event in kernel_events if event['gpu_kernel_op_cat'].lower() == op_cat.lower()]
            print('\n op_cat:', op_cat)
            print('\n custom_call_target:', Counter([event.get('metadata', {}).get('custom_call_target', 'NA') for event in op_events]))
            print('operands:', Counter([str(event.get('metadata', {}).get('operands', '[]')) for event in op_events]))
            print('outputs:', Counter([str(event.get('metadata', {}).get('outputs', '[]')) for event in op_events]))

        # save output to csv
        rows = []
        kernel_events =  [event for event in perf_analyzer.tree.events if event['cat']=='kernel']
        for event in kernel_events:
            _custom_call = event.get('metadata', {}).get('custom_call_target', 'NA')
            _operands = event.get('metadata', {}).get('operands', '[]')
            _op_kernel = event.get('gpu_kernel_op_cat', 'NA')
            _metadata = event.get('metadata', '{}')
            metrics_event = {'name': event['name'],
                                'pid': event['pid'],
                                'UID': event['UID'],
                                'custom_call_target': _custom_call,
                                'operands': _operands,
                                'gpu_kernel_op_cat': _op_kernel,
                                'metadata': _metadata}
            rows.append(metrics_event)
        import pandas as pd
        df = pd.DataFrame(rows)
        df.to_csv('/home/guangphu/perf-profiling/logs/tracelens/jax/trace_analysis_results_kernel_event_metadata.csv')
        #sys.exit(0)

    # Generate & store base DataFrames
    df_gpu_events_averages = perf_analyzer.get_df_gpu_events_averages() 
    dict_dfs['gpu_events_averages']= df_gpu_events_averages
    
    # Generate & store op-specific DataFrames
    kernel_events =  [event for event in perf_analyzer.tree.events if event['cat']=='kernel']
    for op_cat in ['GEMM', 'Conv', 'TE', 'FA FWD', 'FA BWD']: # TraceEventUtils.JaxOpKeys.ClassCategories.keys()
        op_events = [event for event in kernel_events if event['gpu_kernel_op_cat'].lower() == op_cat.lower()]
        df_op_detailed = perf_analyzer.build_df_perf_metrics(op_events, include_kernel_details=False, include_args=False)
        dict_dfs[f"op_{op_cat}_detailed"] = df_op_detailed
        df_op = perf_analyzer.summarize_df_perf_metrics(df_op_detailed, agg_metrics)
        dict_dfs[f"op_{op_cat}"] = df_op

    # Generate & store Dataframes on XLA kernels 
    df_xla_events = perf_analyzer.get_df_kernel_launchers(include_kernel_details=True, gpu_kernel_op_cats=['xla'])
    df_xla_summary = perf_analyzer.get_df_kernel_launchers_summary(df_xla_events)
    dict_dfs['xla_summary']= df_xla_summary # TODO: aggregate according to naming

    # Generate & store Dataframes on Communication nccl/rccl kernels
    df_comm_events = perf_analyzer.get_df_kernel_launchers(include_kernel_details=True, gpu_kernel_op_cats=['nccl', 'rccl'])
    df_comm_summary = perf_analyzer.get_df_kernel_launchers_summary(df_comm_events)
    dict_dfs['comm_summary']= df_comm_summary

    return dict_dfs

def main():
    """
    Usage:
        mkdir -p $log_dir/tracelens/pytorch $log_dir/tracelens/jax 
        python3 TraceLens/Reporting/perf_report.py --profile_path $trace_dir/$trace_pytorch --output_path $log_dir/tracelens/pytorch
        python3 TraceLens/Reporting/perf_report.py --profile_path $trace_dir/$trace_jax --output_path $log_dir/tracelens/jax 
    """
    # check openpyxl is installed
    try:
        import openpyxl
    except ImportError:
        raise ImportError("openpyxl is required to write Excel files for perf report gen. Please install it using 'pip install openpyxl'.")

    parser = argparse.ArgumentParser(description='Process a pytroch JSON trace or JAX xplane.pb profile and generate performance report tables.')
    parser.add_argument('--profile_path', type=str, required=True, help='Path to the trace file: pytorch trace.json or jax xplane.pb')
    parser.add_argument('--gpu_arch_json_path', type=str, default=None, help='Path to the GPU architecture JSON file')
    parser.add_argument("--num_cus", type=str, default=304, help="Number of compute units, MI300X - 304; MI210: 104")
    parser.add_argument("--output_path", type=str, required=True, help="Path to the output folder")
    parser.add_argument("--output_table_formats", type=str, nargs="+", default=[".xlsx", ".csv"], choices=[".xlsx", ".csv"], help="Output table save formats. You can select one or both formats: .xlsx and/or .csv.")
    parser.add_argument("--output_filename", type=str, default="trace_analysis_results", help="Base name for output files")
    args = parser.parse_args()

    # Load the arch json
    gpu_arch_json = None
    if args.gpu_arch_json_path:
        with open(args.gpu_arch_json_path, 'r') as f:
            gpu_arch_json = json.load(f)

    # Analyze trace profile
    assert args.profile_path.endswith('.pt.trace.json') or args.profile_path.endswith('.xplane.pb')
    dict_dfs = {}
    dict_dfs = perf_analysis(args.profile_path, arch=gpu_arch_json, num_cus=args.num_cus)

    # OP Specific analysis on Pytorch trace.json file
    if args.profile_path.endswith('.pt.trace.json'):
        _dfs = perf_pytorch(args.profile_path)
        dict_dfs.update(_dfs)

    # OP Specific analysis on Jax xplane.pb file
    if args.profile_path.endswith('.xplane.pb'):
        _dfs = perf_jax(args.profile_path) 
        dict_dfs.update(_dfs)

    # Save the output
    output_folder = Path(args.output_path)
    output_filename = args.output_filename
    try:
        output_folder.mkdir(parents=True, exist_ok=True)
    except PermissionError:
        print(f"Error: Insufficient permissions to create the directory at {args.output_path}.", file=sys.stderr)
        sys.exit(1)
    except FileNotFoundError:
        print(f"Error: The specified path {args.output_path} is invalid.", file=sys.stderr)
        sys.exit(1)
    for name_df, df in dict_dfs.items():
        suffix = '_' + '_'.join(name_df.lower().split())
        export_data_df(df, output_folder, output_filename, output_table_format=args.output_table_formats, suffix=suffix,)

    print(f"DataFrames successfully written to {args.output_path}")

if __name__ == "__main__":
    main()
