###############################################################################
# Copyright (c) 2024 - 2025 Advanced Micro Devices, Inc. All rights reserved.
#
# See LICENSE for license information.
###############################################################################

import argparse, os, sys
import json
from typing import Optional, Dict
import pandas as pd
import logging

# Configure basic logging to stdout with DEBUG level
logging.basicConfig(
    stream=sys.stdout,  # Output to console
    level=logging.DEBUG,  # Set the minimum log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    format="[%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s",
)

from TraceLens.PerfModel import jax_op_mapping
from TraceLens.TreePerf import TreePerfAnalyzer, JaxTreePerfAnalyzer
from TraceLens.Reporting.reporting_utils import request_install


def perf_analysis(
    profile_path: str,
    agg_metrics=["mean", "median", "std", "min", "max"],
    **kwargs,
) -> Dict[str, pd.DataFrame]:
    """
    Generates a performance report for JAX analysis from a given XPlane profile protobuf file.
    This function processes GPU event statistics and GEMM (General Matrix Multiply) performance data
    from the specified profile file, and exports the results into a dictionary of Dataframes.

    It summarizes GPU events by calculating averages, categorizing events,
    and grouping XLA events by their base names. It also computes the overlapped communication
    time and appends it to the averages DataFrame.

    Args:
        profile_path (str): Path to the input XPlane profile protobuf file generated by JAX.
        agg_metrics (list, optional): List of aggregation metrics to compute for performance summaries.
            Defaults to ["mean", "median", "std", "min", "max"].

    Returns:
        JAX perf dictionary of DataFrames:
            - df_gpu_events_averages (pd.DataFrame): DataFrame containing average times and percentages for various GPU event types, including overlapped communication.
            - df_gpu_events_categorized_mean (pd.DataFrame): DataFrame with categorized GPU event statistics, indexed and renamed for clarity.
            - df_xla_grouped (pd.DataFrame): DataFrame of XLA events grouped by base name, sorted by percentage of total time.
            - df_op_jax_gemm (pd.DataFrame): DataFrame of GEMMs
            - df_op_jax_conv (pd.DataFrame): DataFrame of Convolution kernels
            - df_op_jax_te (pd.DataFrame): DataFrame of TE kernels
    """
    # Get input trace type
    assert profile_path.endswith(".xplane.pb")
    perf_analyzer = JaxTreePerfAnalyzer.from_file(
        profile_filepath=profile_path,
        kernel_metadata_keyword_filters=kwargs.get(
            "kernel_metadata_keyword_filters", None
        ),
    )

    # TODO: add option to use PyTorch Perf Analyzer for PyTorch traces i.e. unify reporting modules.
    """perf_analyzer = TreePerfAnalyzer.from_file(
        profile_filepath=profile_json_path,
        arch=gpu_arch_json,
        python_path=python_path,
        include_unlinked_kernels=include_unlinked_kernels,
    )"""

    # Generate base DataFrames
    df_gpu_timeline = perf_analyzer.get_df_gpu_timeline()
    df_gpu_events_averages = perf_analyzer.get_df_gpu_events_averages()
    df_kernel_launchers = perf_analyzer.get_df_kernel_launchers(
        include_kernel_details=True
    )
    df_kernel_launchers_summary = perf_analyzer.get_df_kernel_launchers_summary(
        df_kernel_launchers
    )
    df_kernel_launchers_summary_by_category = (
        perf_analyzer.get_df_kernel_launchers_summary_by_category(df_kernel_launchers)
    )
    df_kernel_launchers_unique_args = perf_analyzer.get_df_kernel_launchers_unique_args(
        df_kernel_launchers, agg_metrics=agg_metrics, include_pct=True
    )

    # Generate & store Dataframes on selected e.g. XLA kernels
    df_xla_events = perf_analyzer.get_df_kernel_launchers(
        include_kernel_details=True,
        gpu_kernel_op_cats=[
            "Uncategorized Events/XLA",
        ],
    )
    df_xla_perf = perf_analyzer.get_df_xla_perf(df_xla_events)
    df_xla_events_agg_name_col = df_xla_events.copy()
    df_xla_events_agg_name_col["name"] = df_xla_events.name.apply(
        lambda x: "".join([i for i in x if not i.isdigit()])
    )  # remove last part in name e.g. loop_slice_fusion_202 > loop_slice_fusion
    df_xla_summary = perf_analyzer.get_df_kernel_launchers_summary(
        df_xla_events_agg_name_col
    )

    # Store base DataFrames
    dict_name2df = {}
    dict_name2df["gpu_timeline"] = df_gpu_timeline
    dict_name2df["gpu_events_averages"] = df_gpu_events_averages
    dict_name2df["kernel_launchers"] = df_kernel_launchers
    dict_name2df["kernel_launchers_summary"] = df_kernel_launchers_summary
    dict_name2df["kernel_launchers_summary_by_category"] = (
        df_kernel_launchers_summary_by_category
    )
    dict_name2df["kernel_launchers_unique_args"] = df_kernel_launchers_unique_args
    dict_name2df["df_xla_perf"] = df_xla_perf
    dict_name2df["xla_summary"] = df_xla_summary

    # Generate & store perf-model specific DataFrames
    op_events = [
        event for event in perf_analyzer.tree.events if event["cat"] == "kernel"
    ]
    df_op_detailed = perf_analyzer.build_df_perf_metrics(
        op_events, include_kernel_details=True, include_args=True
    )
    for op_cat in [
        "jax_gemm",
        "jax_conv",
        "jax_te",
    ]:  # Alternatively: jax_op_mapping.jax_op_to_perf_model_class_map.keys():
        df_op_perf_model = df_op_detailed[
            df_op_detailed["perf model"].str.contains(op_cat)
        ]
        df_op_perf_model_cleaned = df_op_perf_model.dropna(
            how="all", axis=1
        )  # remove empty columns. Not all perf model classes share the same params.
        df_op = perf_analyzer.summarize_df_perf_metrics(
            df_op_perf_model_cleaned, agg_metrics
        )
        dict_name2df[f"op_{op_cat}"] = df_op
    return dict_name2df


def generate_perf_report_jax(
    profile_path: str,
    output_xlsx_path: Optional[str] = None,
    output_csvs_dir: Optional[str] = None,
    gpu_arch_json_path: Optional[str] = None,
    kernel_metadata_keyword_filters=None,
) -> Dict[str, pd.DataFrame]:
    # Load the arch json
    gpu_arch_json = None
    if gpu_arch_json_path:
        with open(gpu_arch_json_path, "r") as f:
            gpu_arch_json = json.load(f)

    # Analyze trace profile
    dict_name2df = perf_analysis(
        profile_path,
        arch=gpu_arch_json,
        kernel_metadata_keyword_filters=kernel_metadata_keyword_filters,
    )

    # Write all DataFrames to separate sheets in an Excel workbook
    if output_csvs_dir:
        # Ensure the output directory exists
        os.makedirs(output_csvs_dir, exist_ok=True)
        for sheet_name, df in dict_name2df.items():
            csv_path = os.path.join(output_csvs_dir, f"{sheet_name}.csv")
            df.to_csv(csv_path, index=False)
            print(f"DataFrame '{sheet_name}' written to {csv_path}")
    else:
        if output_xlsx_path is None:
            # split input path at 'xplane.pb' and take the first part and append '.xlsx'
            base_path = profile_path.rsplit(".xplane.pb", 1)[0]
            output_xlsx_path = base_path + "_perf_report.xlsx"
        try:
            import openpyxl
        except (ImportError, ModuleNotFoundError) as e:
            print(f"Error importing openpyxl: {e}")
            request_install("openpyxl")

        with pd.ExcelWriter(output_xlsx_path, engine="openpyxl") as writer:
            for sheet_name, df in dict_name2df.items():
                df.to_excel(writer, sheet_name=sheet_name, index=False)
            print(f"DataFrames successfully written to {output_xlsx_path}")

    return dict_name2df


def main():
    """
    Usage:
        mkdir -p $log_dir/tracelens/jax
        python3 TraceLens/Reporting/generate_perf_report_jax.py --profile_path $trace_dir/$trace_jax --output_csvs_dir $log_dir/tracelens/jax
    """

    parser = argparse.ArgumentParser(
        description="Process a JAX xplane.pb profile and generate performance report tables."
    )
    parser.add_argument(
        "--profile_path",
        type=str,
        required=True,
        help="Path to the trace file: pytorch trace.json or jax xplane.pb",
    )
    parser.add_argument(
        "--output_xlsx_path",
        type=str,
        default=None,
        help="Path to the output Excel file",
    )
    parser.add_argument(
        "--output_csvs_dir",
        type=str,
        default=None,
        help="Directory to save output CSV files",
    )

    # Optional arguments
    parser.add_argument(
        "--gpu_arch_json_path",
        type=str,
        default=None,
        help="Path to the GPU architecture JSON file",
    )
    parser.add_argument(
        "--num_cus",
        type=str,
        default=304,
        help="Number of compute units, MI300X - 304; MI210: 104",
    )
    parser.add_argument(
        "--kernel_metadata_keyword_filters",
        type=str,
        nargs="+",
        default=None,
        help="Kernel metadata keyword filters, performance analysis is computed only for the events containing the kerword in the metadata e.g. in framework name scope, e.g. --kernel_metadata_keyword_filters remat checkpoint",
    )
    args = parser.parse_args()

    generate_perf_report_jax(
        profile_path=args.profile_path,
        output_xlsx_path=args.output_xlsx_path,
        output_csvs_dir=args.output_csvs_dir,
        gpu_arch_json_path=args.gpu_arch_json_path,
        kernel_metadata_keyword_filters=args.kernel_metadata_keyword_filters,
    )


if __name__ == "__main__":
    main()
