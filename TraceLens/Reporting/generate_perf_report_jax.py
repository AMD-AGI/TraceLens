import argparse, os, sys
import json
from pathlib import Path
import pandas as pd

from TraceLens.PerfModel import jax_op_mapping
from TraceLens.TreePerf import TreePerfAnalyzer, JaxTreePerfAnalyzer
from TraceLens.Reporting.reporting_utils import request_install


def perf_analysis(
    profile_path: str,
    arch=None,
    agg_metrics=["mean", "median", "std", "min", "max"],
    *args,
    **kwargs,
) -> dict:
    """
    Generates a performance report for JAX analysis from a given XPlane profile protobuf file.
    This function processes GPU event statistics and GEMM (General Matrix Multiply) performance data
    from the specified profile file, and exports the results into a dictionary of Dataframes.

    It summarizes GPU events by calculating averages, categorizing events,
    and grouping XLA events by their base names. It also computes the overlapped communication
    time and appends it to the averages DataFrame.

    Args:
        profile_path (str): Path to the input XPlane profile protobuf file generated by JAX.
        num_cus (int, optional): Number of compute units (CUs) for the GPU architecture. Defaults to 304.

    Returns:
        dict:
            - df_gpu_events_averages (pd.DataFrame): DataFrame containing average times and percentages for various GPU event types, including overlapped communication.
            - df_gpu_events_categorized_mean (pd.DataFrame): DataFrame with categorized GPU event statistics, indexed and renamed for clarity.
            - df_xla_grouped (pd.DataFrame): DataFrame of XLA events grouped by base name, sorted by percentage of total time.
            - df_gemms_detailed(pd.DataFrame): DataFrame of GEMMs
    """
    # Get input trace type
    assert profile_path.endswith(".xplane.pb")
    perf_analyzer = JaxTreePerfAnalyzer.from_file(profile_filepath=profile_path)

    dict_dfs = {}

    # Generate base DataFrames
    df_gpu_timeline = perf_analyzer.get_df_gpu_timeline()
    df_gpu_events_averages = perf_analyzer.get_df_gpu_events_averages()
    df_kernel_launchers = perf_analyzer.get_df_kernel_launchers(
        include_kernel_details=True
    )
    df_kernel_launchers_summary = perf_analyzer.get_df_kernel_launchers_summary(
        df_kernel_launchers
    )
    df_kernel_launchers_summary_by_category = (
        perf_analyzer.get_df_kernel_launchers_summary_by_category(df_kernel_launchers)
    )
    df_kernel_launchers_unique_args = perf_analyzer.get_df_kernel_launchers_unique_args(
        df_kernel_launchers, agg_metrics=agg_metrics, include_pct=True
    )

    # Generate & store Dataframes on selected e.g. XLA kernels
    df_xla_events = perf_analyzer.get_df_kernel_launchers(
        include_kernel_details=True,
        gpu_kernel_op_cats=[
            "Uncategorized Events/XLA",
        ],
    )
    df_xla_events_agg_name_col = df_xla_events.copy()
    df_xla_events_agg_name_col["name"] = df_xla_events.name.apply(
        lambda x: "".join([i for i in x if not i.isdigit()])
    )  # remove last part in name e.g. loop_slice_fusion_202 > loop_slice_fusion
    df_xla_summary = perf_analyzer.get_df_kernel_launchers_summary(
        df_xla_events_agg_name_col
    )

    # Store base DataFrames
    dict_dfs["gpu_timeline"] = df_gpu_timeline
    dict_dfs["gpu_events_averages"] = df_gpu_events_averages
    dict_dfs["kernel_launchers"] = df_kernel_launchers
    dict_dfs["kernel_launchers_summary"] = df_kernel_launchers_summary
    dict_dfs["kernel_launchers_summary_by_category"] = (
        df_kernel_launchers_summary_by_category
    )
    dict_dfs["kernel_launchers_unique_args"] = df_kernel_launchers_unique_args
    dict_dfs["xla_summary"] = df_xla_summary

    # Generate & store perf-model specific DataFrames
    op_events = [
        event for event in perf_analyzer.tree.events if event["cat"] == "kernel"
    ]
    df_op_detailed = perf_analyzer.build_df_perf_metrics(
        op_events, include_kernel_details=True, include_args=True
    )
    for op_cat in [
        "jax_gemm",
        "jax_conv",
        "jax_te",
    ]:  # jax_op_mapping.jax_op_to_perf_model_class_map.keys():
        df_op_perf_model = df_op_detailed[
            df_op_detailed["perf model"].str.contains(op_cat)
        ]
        df_op_perf_model_cleaned = df_op_perf_model.dropna(
            how="all", axis=1
        )  # remove empty columns. Not all perf model classes share the same params.
        df_op = perf_analyzer.summarize_df_perf_metrics(
            df_op_perf_model_cleaned, agg_metrics
        )
        dict_dfs[f"op_{op_cat}"] = df_op
    return dict_dfs


def generate_perf_report_jax(
    profile_path: str,
    gpu_arch_json_path=None,
    num_cus=None,
    output_path="./",
    output_table_formats=[".csv"],
    output_filename="trace_analysis_results",
):
    # Load the arch json
    gpu_arch_json = None
    if gpu_arch_json_path:
        with open(gpu_arch_json_path, "r") as f:
            gpu_arch_json = json.load(f)

    # Analyze trace profile
    assert profile_path.endswith(".xplane.pb")
    dict_dfs = {}
    dict_dfs = perf_analysis(profile_path, arch=gpu_arch_json, num_cus=num_cus)

    # Save the output
    output_folder_path = Path(output_path)
    output_filename = output_filename
    try:
        output_folder_path.mkdir(parents=True, exist_ok=True)
    except PermissionError:
        print(
            f"Error: Insufficient permissions to create the directory at {output_path}.",
            file=sys.stderr,
        )
        sys.exit(1)
    except FileNotFoundError:
        print(f"Error: The specified path {output_path} is invalid.", file=sys.stderr)
        sys.exit(1)

    # Write all DataFrames to separate csv
    if "csv" in output_table_formats:
        for name_df, data_df in dict_dfs.items():
            suffix = "_" + "_".join(name_df.lower().split())
            csv_path = os.path.join(
                output_path, f"{output_filename + suffix}.csv"
            )  # output_folder_path.joinpath(output_filename + suffix).with_suffix(".csv")
            data_df.to_csv(csv_path, index=False)
            print(f"DataFrame '{name_df}' written to {output_path}")

    # Write all DataFrames to separate sheets in one Excel workbook
    if "xlsx" in output_table_formats:
        try:
            import openpyxl
        except (ImportError, ModuleNotFoundError) as e:
            print(f"Error importing openpyxl: {e}")
            request_install("openpyxl")

        output_xlsx_path = os.path.join(output_path, f"{output_filename}.xlsx")
        with pd.ExcelWriter(output_xlsx_path, engine="openpyxl") as writer:
            for name_df, data_df in dict_dfs.items():
                data_df.to_excel(writer, sheet_name=name_df, index=False)
            print(f"DataFrames successfully written to {output_xlsx_path}")


def main():
    """
    Usage:
        mkdir -p $log_dir/tracelens/pytorch $log_dir/tracelens/jax
        python3 TraceLens/Reporting/perf_report.py --profile_path $trace_dir/$trace_pytorch --output_path $log_dir/tracelens/pytorch
        python3 TraceLens/Reporting/perf_report.py --profile_path $trace_dir/$trace_jax --output_path $log_dir/tracelens/jax
    """
    # check openpyxl is installed
    try:
        import openpyxl
    except ImportError:
        raise ImportError(
            "openpyxl is required to write Excel files for perf report gen. Please install it using 'pip install openpyxl'."
        )

    parser = argparse.ArgumentParser(
        description="Process a pytroch JSON trace or JAX xplane.pb profile and generate performance report tables."
    )
    parser.add_argument(
        "--profile_path",
        type=str,
        required=True,
        help="Path to the trace file: pytorch trace.json or jax xplane.pb",
    )
    parser.add_argument(
        "--gpu_arch_json_path",
        type=str,
        default=None,
        help="Path to the GPU architecture JSON file",
    )
    parser.add_argument(
        "--num_cus",
        type=str,
        default=304,
        help="Number of compute units, MI300X - 304; MI210: 104",
    )
    parser.add_argument(
        "--output_path", type=str, required=True, help="Path to the output folder"
    )
    parser.add_argument(
        "--output_table_formats",
        type=str,
        nargs="+",
        default=[
            "csv",
        ],
        choices=["xlsx", "csv"],
        help="Output table save formats. You can select one or both formats: .xlsx and/or .csv.",
    )
    parser.add_argument(
        "--output_filename",
        type=str,
        default="trace_analysis_results",
        help="Base name for output files",
    )

    args = parser.parse_args()
    generate_perf_report_jax(
        profile_path=args.profile_path,
        gpu_arch_json_path=args.gpu_arch_json_path,
        num_cus=args.num_cus,
        output_path=args.output_path,
        output_table_formats=args.output_table_formats,
        output_filename=args.output_filename,
    )


if __name__ == "__main__":
    main()
