{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Profiling Tutorial (Quick Start)\n",
    "\n",
    "The goal of this tutorial is to help you quickly get hands-on with **PyTorch's built-in profiler**, enabling you to analyze and diagnose performance bottlenecks in deep learning models.\n",
    "\n",
    "By the end of this guide, you'll be able to:\n",
    "- Configure and run the profiler for both short and long training runs.\n",
    "- Understand key profiler options such as `record_shapes`, `with_stack`.\n",
    "- Generate Chrome trace timelines. \n",
    "- Use profiler scheduling and trace handlers to profile long-running training jobs efficiently.\n",
    "\n",
    "We'll use a simple ResNet18 model with synthetic data to stay focused on profiling behavior—not model convergence or accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1  Environment & model setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NpwSK6iUtyxA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "device = \"cuda\"\n",
    "dtype  = torch.bfloat16\n",
    "model  = models.resnet18().to(device).to(dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2  Random data & **one reusable training step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0hmWp_N2knn"
   },
   "outputs": [],
   "source": [
    "B, C, H, W   = 5, 3, 224, 224\n",
    "num_classes  = 1000\n",
    "\n",
    "dummy_input  = torch.randn(B, C, H, W,\n",
    "                           device=device, dtype=dtype)\n",
    "dummy_target = torch.randn(B, num_classes,\n",
    "                           device=device, dtype=dtype)\n",
    "\n",
    "def train_step():\n",
    "    # Single forward + backward pass.\n",
    "    output = model(dummy_input)\n",
    "    loss   = torch.nn.functional.mse_loss(output, dummy_target)\n",
    "    loss.backward()\n",
    "\n",
    "# test it out\n",
    "train_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3  Warm‑up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5avS7Ow167VB"
   },
   "outputs": [],
   "source": [
    "def warm_up(iters: int = 10):\n",
    "    for _ in range(iters):\n",
    "        train_step()\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4  Basic profiling pass – capture a timeline\n",
    "This section shows how to capture a minimal execution trace using PyTorch's profiler. The `activities` argument specifies which device events to track:\n",
    "\n",
    "* `ProfilerActivity.CPU` – records CPU-side operator execution.\n",
    "* `ProfilerActivity.CUDA` – records GPU kernel launches and durations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4a53bMAM7IyY"
   },
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity\n",
    "\n",
    "warm_up()\n",
    "with profile(activities=[ProfilerActivity.CPU,\n",
    "                         ProfilerActivity.CUDA]) as p:\n",
    "    train_step()\n",
    "p.export_chrome_trace(\"trace_minimal.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open these json files in **https://ui.perfetto.dev/**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5  Recording tensor shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_up()\n",
    "with profile(activities=[ProfilerActivity.CPU,\n",
    "                         ProfilerActivity.CUDA],\n",
    "             record_shapes=True) as p:\n",
    "    train_step()\n",
    "p.export_chrome_trace(\"trace_shapes.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6  Including Python stack traces (larger trace files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_up()\n",
    "with profile(activities=[ProfilerActivity.CPU,\n",
    "                         ProfilerActivity.CUDA],\n",
    "             record_shapes=True,\n",
    "             with_stack=True) as p:\n",
    "    train_step()\n",
    "p.export_chrome_trace(\"trace_stack.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ `with_stack=True` notably increases the **trace file size** and it is recommended to switch it off for large profiling runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7  Profiling long trainings with a **schedule**\n",
    "\n",
    "When profiling longer training runs, capturing every iteration is too expensive and unnecessary. PyTorch's `schedule()` allows fine-grained control over when to record and when to skip. Here's what the arguments mean:\n",
    "\n",
    "* `wait`: Number of iterations to skip before profiling begins.\n",
    "* `warmup`: Profiler enabled but not yet saving traces — it measures kernel timings internally so the system reaches steady‑state, but these iterations are discarded.\n",
    "* `active`: Number of iterations to record traces for.\n",
    "* `repeat`: How many times to repeat the (wait → warmup → active) cycle.\n",
    "\n",
    "This lets you profile windows of activity in long runs without generating massive trace files.\n",
    "\n",
    "By default, `repeat=0`, which means the profiler will continue executing (wait → warmup → active) cycles **indefinitely** until the job ends. Setting `repeat=1` means only one such cycle is run, and `repeat=2` runs two cycles, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsDDCnBTu5l7"
   },
   "outputs": [],
   "source": [
    "from torch.profiler import schedule\n",
    "\n",
    "sched_wait, sched_warmup, sched_active, sched_repeat = 10, 5, 3, 2\n",
    "sched = schedule(wait=sched_wait, \n",
    "                 warmup=sched_warmup, \n",
    "                 active=sched_active, \n",
    "                 repeat=sched_repeat)\n",
    "\n",
    "def trace_handler(p):\n",
    "    # this is called at the end of the active window\n",
    "    # ``p.step_num`` is the last iteration of the *active* window.\n",
    "    start = p.step_num - sched_active + 1\n",
    "    end   = p.step_num\n",
    "    p.export_chrome_trace(f\"trace_iter{start}_{end}.json\")\n",
    "\n",
    "with profile(activities=[ProfilerActivity.CPU,\n",
    "                         ProfilerActivity.CUDA],\n",
    "             schedule=sched,\n",
    "             record_shapes=True,\n",
    "             with_stack=True,\n",
    "             on_trace_ready=trace_handler) as p:\n",
    "    warm_up()\n",
    "    for _ in range(100):\n",
    "        train_step()\n",
    "        p.step()                   # marks iteration boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you undersand the details of collecting the trace we can move on to the analysis. ![Torch Profiling Analysis docs](./profiling_analysis.md)  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
