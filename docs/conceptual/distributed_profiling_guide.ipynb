{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "538e8fa4",
   "metadata": {},
   "source": [
    "### 🧩 **PyTorch Profiling a Distributed Workload**\n",
    "\n",
    "The goal of this tutorial is to understand the **core concepts behind profiling a distributed workload**.  \n",
    "It directly builds on the previous tutorial that covered profiling a single-GPU run.\n",
    "\n",
    "Here we focus on what changes when a workload runs under **PyTorch Distributed (DDP or otherwise)** — not on the mechanics of DDP itself, but on how profiling behaves when multiple processes participate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be666d",
   "metadata": {},
   "source": [
    "### ⚙️ How Distributed Profiling Works\n",
    "\n",
    "This example uses **`torchrun`** to launch the job. The key concept is that `torchrun` starts **one identical Python process for every GPU** you are using.\n",
    "\n",
    "When you profile this distributed workload, you are really profiling **multiple identical programs** at the same time, each running on its own GPU.\n",
    "\n",
    "\n",
    "\n",
    "#### What \"Rank\" Actually Means\n",
    "\n",
    "To coordinate, each process needs a unique identity. `torchrun` provides this identity using environment variables, which we call **\"rank.\"**\n",
    "\n",
    "* **`WORLD_SIZE`**: The **total number** of processes in the entire job (e.g., 2 nodes $\\times$ 8 GPUs/node = 16).\n",
    "* **`RANK`**: The **global ID** for this specific process, from `0` to `WORLD_SIZE - 1`.\n",
    "* **`LOCAL_RANK`**: The **local GPU index** *on this specific machine* (e.g., `0`, `1`, ... `7` for an 8-GPU node).\n",
    "\n",
    "Every process runs the *same Python program*, but it uses these rank variables to figure out what to do:\n",
    "* It uses `LOCAL_RANK` to select its GPU (e.g., `cuda:0`, `cuda:1`, …).\n",
    "* It uses `RANK` to participate in collective operations (like gradient sync).\n",
    "\n",
    "---\n",
    "\n",
    "#### Why naive profiling breaks\n",
    "If every rank writes to the same `trace.json`, the independent processes will **race and overwrite** each other’s output.\n",
    "\n",
    "**To avoid file races (required):**\n",
    "- **Use a unique filename per rank**, such as `rank{rank}_iter{start}_{end}.json`.  \n",
    "  This ensures that concurrent processes never write to the same file.\n",
    "\n",
    "**To reduce trace volume (optional):**\n",
    "- **Enable profiling only for selected ranks** (e.g., `\"0\"` or `\"0,3\"`).  \n",
    "  Every rank still executes the full workload, but only the chosen ranks record traces—keeping the data manageable and export overhead low.\n",
    "\n",
    "The helper **`make_profiler_ctx()`** in the first cell implements both of these ideas:\n",
    "- It filters which ranks actually profile.  \n",
    "- It embeds the rank number into the trace filename so outputs are always unique.\n",
    "\n",
    "\n",
    "> 🔗 Optional references:  \n",
    "> • [DDP Tutorial — PyTorch](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)  \n",
    "> • [`torchrun` launcher docs](https://pytorch.org/docs/stable/elastic/run.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec2366b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train_ddp.py\n",
    "import os\n",
    "import contextlib\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.profiler import profile, ProfilerActivity, schedule as profiler_schedule\n",
    "\n",
    "###############################################################################\n",
    "# DDP init / cleanup\n",
    "###############################################################################\n",
    "\n",
    "def setup():\n",
    "    # torchrun sets these:\n",
    "    #   RANK, WORLD_SIZE, LOCAL_RANK, MASTER_ADDR, MASTER_PORT\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    dist.init_process_group(\n",
    "        backend=\"nccl\",\n",
    "        rank=rank,\n",
    "        world_size=world_size,\n",
    "        device_id=local_rank,\n",
    "    )\n",
    "\n",
    "    return rank, world_size, local_rank\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Core workload\n",
    "###############################################################################\n",
    "\n",
    "def make_model_and_data(rank):\n",
    "    \"\"\"\n",
    "    Match the profiling guide:\n",
    "      - model: torchvision.models.resnet18()\n",
    "      - dtype: bfloat16 on GPU\n",
    "      - dummy_input:  [5, 3, 224, 224]\n",
    "      - dummy_target: [5, 1000]\n",
    "    \"\"\"\n",
    "    device = torch.device(f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dtype = torch.bfloat16  # same as profiling guide\n",
    "    model = models.resnet18().to(device).to(dtype)\n",
    "    ddp_model = DDP(model, device_ids=[rank] if device.type == \"cuda\" else None)\n",
    "    optimizer = torch.optim.SGD(ddp_model.parameters(), lr=1e-3)\n",
    "\n",
    "    # fixed random batch (no dataloader), same as guide\n",
    "    B, C, H, W = 10, 3, 224, 224\n",
    "    num_classes = 1000\n",
    "\n",
    "    dummy_input = torch.randn(B, C, H, W, device=device, dtype=dtype)\n",
    "    dummy_target = torch.randn(B, num_classes, device=device, dtype=dtype)\n",
    "\n",
    "    return ddp_model, optimizer, dummy_input, dummy_target, device\n",
    "\n",
    "\n",
    "def train_step(ddp_model, optimizer, X, Y):\n",
    "    \"\"\"\n",
    "    Single step:\n",
    "      forward -> mse_loss -> backward -> step\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    out = ddp_model(X)\n",
    "    loss = F.mse_loss(out, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Profiler setup (same schedule as the profiling guide)\n",
    "###############################################################################\n",
    "\n",
    "def make_profiler_ctx(rank, profile_ranks, traces_dir):\n",
    "    \"\"\"\n",
    "    We use the same schedule pattern as in the notebook:\n",
    "      wait=10, warmup=5, active=3, repeat=1\n",
    "    Every time an 'active' window finishes, we dump a Chrome trace\n",
    "    file for that rank using export_chrome_trace.\n",
    "    \"\"\"\n",
    "    class NullProfiler:\n",
    "        def __enter__(self):\n",
    "            return self\n",
    "        def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "            pass\n",
    "        def step(self):\n",
    "            pass\n",
    "\n",
    "    # Decide if this rank should actually profile\n",
    "    if profile_ranks != \"all\":\n",
    "        profile_ranks_int = {int(x) for x in profile_ranks.split(\",\") if x}\n",
    "        if rank not in profile_ranks_int:\n",
    "            return NullProfiler()\n",
    "\n",
    "    os.makedirs(traces_dir, exist_ok=True)\n",
    "\n",
    "    sched = profiler_schedule(\n",
    "        wait=10,\n",
    "        warmup=5,\n",
    "        active=3,\n",
    "        repeat=1,\n",
    "    )\n",
    "\n",
    "    def trace_handler(p):\n",
    "        end_iter = p.step_num\n",
    "        start_iter = end_iter - 3 + 1  # active window length = 3\n",
    "        trace_path = os.path.join(traces_dir, f\"rank{rank}_iter{start_iter}_{end_iter}.json\")\n",
    "        p.export_chrome_trace(trace_path)\n",
    "\n",
    "    return profile(\n",
    "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "        schedule=sched,\n",
    "        record_shapes=True,\n",
    "        with_stack=True,\n",
    "        on_trace_ready=trace_handler,\n",
    "    )\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Per-rank entry point\n",
    "###############################################################################\n",
    "\n",
    "def ddp_worker(rank, world_size, profile_ranks, traces_dir):\n",
    "    rank, world_size, local_rank = setup()\n",
    "    print(f\"[rank {rank}] starting worker\")\n",
    "\n",
    "    ddp_model, optimizer, dummy_input, dummy_target, device = make_model_and_data(rank)\n",
    "\n",
    "    total_steps = 100\n",
    "    \n",
    "    prof = make_profiler_ctx(rank, profile_ranks, traces_dir)\n",
    "    with prof:\n",
    "        for step_idx in range(total_steps):\n",
    "            loss = train_step(ddp_model, optimizer, dummy_input, dummy_target)\n",
    "            prof.step()\n",
    "            if rank == 0 and step_idx%10 ==0:\n",
    "                print(f\"[rank {rank}] step {step_idx}/{total_steps} \"\n",
    "                        f\"loss={loss.item():.4f}\", flush=True)\n",
    "\n",
    "    # sync before cleanup\n",
    "    dist.barrier(device_ids=[rank])\n",
    "    cleanup()\n",
    "    print(f\"[rank {rank}] finished worker\")\n",
    "\n",
    "def main():\n",
    "    # torchrun sets these per process\n",
    "    rank = int(os.environ[\"RANK\"])\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "\n",
    "    # you can keep these as constants / CLI args if you want\n",
    "    profile_ranks = \"0, 2, 4\"     # or \"all\", or whatever you were passing before\n",
    "    traces_dir = \"./traces\"\n",
    "\n",
    "    ddp_worker(rank, world_size, profile_ranks, traces_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6dcdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "num_procs = 8  # how many GPUs / ranks you want\n",
    "script_path = \"train_ddp.py\"  # path to your script\n",
    "\n",
    "cmd = [\n",
    "    sys.executable,\n",
    "    \"-m\", \"torch.distributed.run\",   # this is effectively torchrun\n",
    "    f\"--nproc_per_node={num_procs}\",\n",
    "    script_path,\n",
    "]\n",
    "\n",
    "print(\"Launching:\", \" \".join(cmd))\n",
    "\n",
    "result = subprocess.run(cmd, capture_output=False, check=False)\n",
    "print(\"Return code:\", result.returncode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a0345a",
   "metadata": {},
   "source": [
    "You should now be able to see the generated trace files in the `./traces` directory.  \n",
    "Each profiled rank writes its own Chrome trace file (for example, `rank0_iter10_12.json`, `rank3_iter10_12.json`, etc.), depending on the `profile_ranks` selection.\n",
    "\n",
    "---\n",
    "\n",
    "### 🌐 Multi-node note\n",
    "\n",
    "For multi-node runs, **nothing changes conceptually** from the profiler’s point of view.  \n",
    "If you have four nodes with eight GPUs each (32 total ranks):\n",
    "\n",
    "- The global ranks will simply span **0 → 31** across all nodes.  \n",
    "- The same rank-filtering and unique filename logic still applies.  \n",
    "- Each process writes its trace to the **storage path accessible to that process**.\n",
    "\n",
    "If you’re writing to **local disk paths** (e.g., `./traces`), each node will contain only the trace files for its local ranks.  \n",
    "For example:\n",
    "- Node 0 → `rank0–7` traces  \n",
    "- Node 1 → `rank8–15` traces  \n",
    "- …and so on.  \n",
    "\n",
    "If instead you write to a **shared NFS or network-mounted directory**, all ranks’ traces will appear in one place, since all nodes share the same storage.  \n",
    "That’s the only difference—profiling logic itself remains identical.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
